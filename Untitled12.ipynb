{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSymCIC7Qwkz"
      },
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYfcc5AiiMZ8"
      },
      "source": [
        "!git clone https://github.com/NonniHlolli/cycleNew.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KCgNsqkljGe"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/cycleNew')\n",
        "!pip install visdom\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWMihn5E_qJL"
      },
      "source": [
        "! npm install -g localtunnel\n",
        "get_ipython().system_raw('python3 -m pip install visdom')\n",
        "get_ipython().system_raw('python3 -m visdom.server -port 8097 >> visdomlog.txt 2>&1 &')   \n",
        "get_ipython().system_raw('lt --port 8097 >> url.txt 2>&1 &')   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "zItAaYKE-UEZ",
        "outputId": "c610002c-b0fb-453a-c461-c036d759a3bd"
      },
      "source": [
        "import visdom  \n",
        "from google.colab import output\n",
        "output.serve_kernel_port_as_iframe(8097)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = url + path;\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    element.appendChild(iframe);\n",
              "  })(8097, \"/\", \"100%\", \"400\", false, window.element)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiVEnT2aw1cn"
      },
      "source": [
        "import itertools\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "import torch\n",
        "from utilities import *\n",
        "from data import *\n",
        "from options import *\n",
        "from gener import *\n",
        "from discr import Discriminator\n",
        "from residBlock import *\n",
        "\n",
        "opt = Options(decay_epoch=5,dataroot = 'cycleNew/apple2orange/', n_cpu = 2, cuda = False, n_epochs=10 )\n",
        "\n",
        "###### Definition of variables ######\n",
        "# Networks\n",
        "netG_A2B = Generator(opt.input_nc, opt.output_nc)\n",
        "netG_B2A = Generator(opt.output_nc, opt.input_nc)\n",
        "netD_A = Discriminator(opt.input_nc)\n",
        "netD_B = Discriminator(opt.output_nc)\n",
        "\n",
        "if opt.cuda:\n",
        "    netG_A2B.cuda()\n",
        "    netG_B2A.cuda()\n",
        "    netD_A.cuda()\n",
        "    netD_B.cuda()\n",
        "\n",
        "netG_A2B.apply(weights_init_normal)\n",
        "netG_B2A.apply(weights_init_normal)\n",
        "netD_A.apply(weights_init_normal)\n",
        "netD_B.apply(weights_init_normal)\n",
        "\n",
        "# Lossess\n",
        "criterion_GAN = torch.nn.MSELoss()\n",
        "criterion_cycle = torch.nn.L1Loss()\n",
        "criterion_identity = torch.nn.L1Loss()\n",
        "\n",
        "# Optimizers & LR schedulers\n",
        "optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n",
        "                                lr=opt.lr, betas=(0.5, 0.999))\n",
        "optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n",
        "optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n",
        "\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n",
        "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n",
        "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n",
        "\n",
        "# Inputs & targets memory allocation\n",
        "Tensor = torch.cuda.FloatTensor if opt.cuda else torch.Tensor\n",
        "input_A = Tensor(opt.batchSize, opt.input_nc, opt.size, opt.size)\n",
        "input_B = Tensor(opt.batchSize, opt.output_nc, opt.size, opt.size)\n",
        "target_real = Variable(Tensor(opt.batchSize).fill_(1.0), requires_grad=False)\n",
        "target_fake = Variable(Tensor(opt.batchSize).fill_(0.0), requires_grad=False)\n",
        "\n",
        "fake_A_buffer = ReplayBuffer()\n",
        "fake_B_buffer = ReplayBuffer()\n",
        "\n",
        "# Dataset loader\n",
        "transforms_ =[ #transforms.Resize(int(opt.size*1.12), Image.BICUBIC), \n",
        "                #transforms.RandomCrop(opt.size), \n",
        "                #transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))]\n",
        "dataloader = DataLoader(ImageDataset(opt.dataroot, transforms_=transforms_, unaligned=True), \n",
        "                        batch_size=opt.batchSize, shuffle=True, num_workers=opt.n_cpu)\n",
        "\n",
        "# Loss plot\n",
        "logger = Logger(opt.n_epochs, len(dataloader))\n",
        "###################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01-OTazm6SLW"
      },
      "source": [
        "\n",
        "\n",
        "###### Training ######\n",
        "for epoch in range(opt.epoch, opt.n_epochs):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        # Set model input\n",
        "        real_A = Variable(input_A.copy_(batch['A']))\n",
        "        real_B = Variable(input_B.copy_(batch['B']))\n",
        "        ###### Generators A2B and B2A ######\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Identity loss\n",
        "        # G_A2B(B) should equal B if real B is fed\n",
        "        same_B = netG_A2B(real_B)\n",
        "        loss_identity_B = criterion_identity(same_B, real_B)*5.0\n",
        "        # G_B2A(A) should equal A if real A is fed\n",
        "        same_A = netG_B2A(real_A)\n",
        "        loss_identity_A = criterion_identity(same_A, real_A)*5.0\n",
        "\n",
        "        # GAN loss\n",
        "        fake_B = netG_A2B(real_A)\n",
        "        pred_fake = netD_B(fake_B)\n",
        "        loss_GAN_A2B = criterion_GAN(pred_fake, target_real)\n",
        "\n",
        "        fake_A = netG_B2A(real_B)\n",
        "        pred_fake = netD_A(fake_A)\n",
        "        loss_GAN_B2A = criterion_GAN(pred_fake, target_real)\n",
        "        \n",
        "\n",
        "        # Cycle loss\n",
        "        recovered_A = netG_B2A(fake_B)\n",
        "        loss_cycle_ABA = criterion_cycle(recovered_A, real_A)*10.0\n",
        "\n",
        "        recovered_B = netG_A2B(fake_A)\n",
        "        loss_cycle_BAB = criterion_cycle(recovered_B, real_B)*10.0\n",
        "\n",
        "        # Total loss\n",
        "        loss_G = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
        "        loss_G.backward()\n",
        "        \n",
        "        optimizer_G.step()\n",
        "        ###################################\n",
        "\n",
        "        ###### Discriminator A ######\n",
        "        optimizer_D_A.zero_grad()\n",
        "\n",
        "        # Real loss\n",
        "        pred_real = netD_A(real_A)\n",
        "        loss_D_real = criterion_GAN(pred_real, target_real)\n",
        "\n",
        "        # Fake loss\n",
        "        fake_A = fake_A_buffer.push_and_pop(fake_A)\n",
        "        pred_fake = netD_A(fake_A.detach())\n",
        "        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
        "\n",
        "        # Total loss\n",
        "        loss_D_A = (loss_D_real + loss_D_fake)*0.5\n",
        "        loss_D_A.backward()\n",
        "\n",
        "        optimizer_D_A.step()\n",
        "        ###################################\n",
        "\n",
        "        ###### Discriminator B ######\n",
        "        optimizer_D_B.zero_grad()\n",
        "\n",
        "        # Real loss\n",
        "        pred_real = netD_B(real_B)\n",
        "        loss_D_real = criterion_GAN(pred_real, target_real)\n",
        "        \n",
        "        # Fake loss\n",
        "        fake_B = fake_B_buffer.push_and_pop(fake_B)\n",
        "        pred_fake = netD_B(fake_B.detach())\n",
        "        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
        "\n",
        "        # Total loss\n",
        "        loss_D_B = (loss_D_real + loss_D_fake)*0.5\n",
        "        loss_D_B.backward()\n",
        "\n",
        "        optimizer_D_B.step()\n",
        "        ###################################\n",
        "\n",
        "        # Progress report (http://localhost:8097)\n",
        "        losses = {'loss_G': loss_G.item(), 'loss_G_identity': (loss_identity_A.item() + loss_identity_B.item()), 'loss_G_GAN': (loss_GAN_A2B.item() + loss_GAN_B2A.item()),\n",
        "                    'loss_G_cycle': (loss_cycle_ABA.item() + loss_cycle_BAB.item()), 'loss_D': (loss_D_A.item() + loss_D_B.item())}\n",
        "        images={'real_A': real_A, 'real_B': real_B, 'fake_A': fake_A, 'fake_B': fake_B}\n",
        "        logger.log(losses, images)\n",
        "\n",
        "    # Update learning rates\n",
        "    lr_scheduler_G.step()\n",
        "    lr_scheduler_D_A.step()\n",
        "    lr_scheduler_D_B.step()\n",
        "\n",
        "    # Save models checkpoints\n",
        "    torch.save(netG_A2B.state_dict(), 'output/netG_A2B.pth')\n",
        "    torch.save(netG_B2A.state_dict(), 'output/netG_B2A.pth')\n",
        "    torch.save(netD_A.state_dict(), 'output/netD_A.pth')\n",
        "    torch.save(netD_B.state_dict(), 'output/netD_B.pth')\n",
        "###################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOckUT_7DmEB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}