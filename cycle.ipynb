{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYfcc5AiiMZ8",
        "outputId": "ac7d296b-6eb3-4405-8a62-c4c3434c4add"
      },
      "source": [
        "!git clone https://github.com/NonniHlolli/cycleNew.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cycleNew'...\n",
            "remote: Enumerating objects: 2553, done.\u001b[K\n",
            "remote: Counting objects: 100% (2553/2553), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2544/2544), done.\u001b[K\n",
            "remote: Total 2553 (delta 16), reused 2538 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2553/2553), 73.99 MiB | 37.20 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KCgNsqkljGe",
        "outputId": "098d9aba-42d8-4702-dedf-952b59a3b0f5"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/cycleNew')\n",
        "!pip install visdom"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: visdom in /usr/local/lib/python3.7/dist-packages (0.1.8.9)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from visdom) (7.1.2)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.7/dist-packages (from visdom) (1.32)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom) (22.0.3)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.7/dist-packages (from visdom) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom) (1.4.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom) (5.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from visdom) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom) (2.23.0)\n",
            "Requirement already satisfied: torchfile in /usr/local/lib/python3.7/dist-packages (from visdom) (0.1.0)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from visdom) (1.19.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.7/dist-packages (from jsonpatch->visdom) (2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (2020.12.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiVEnT2aw1cn",
        "outputId": "d505aadf-2b4f-4409-99bd-1270fa882d54"
      },
      "source": [
        "import itertools\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "import torch\n",
        "from utilities import *\n",
        "from data import *\n",
        "from options import *\n",
        "from gener import *\n",
        "from discr import Discriminator\n",
        "from residBlock import *\n",
        "\n",
        "opt = Options(decay_epoch=10,dataroot = 'cycleNew/apple2orange/', n_cpu = 2)\n",
        "\n",
        "###### Definition of variables ######\n",
        "# Networks\n",
        "netG_A2B = Generator(opt.input_nc, opt.output_nc)\n",
        "netG_B2A = Generator(opt.output_nc, opt.input_nc)\n",
        "netD_A = Discriminator(opt.input_nc)\n",
        "netD_B = Discriminator(opt.output_nc)\n",
        "\n",
        "if opt.cuda:\n",
        "    netG_A2B.cuda()\n",
        "    netG_B2A.cuda()\n",
        "    netD_A.cuda()\n",
        "    netD_B.cuda()\n",
        "\n",
        "netG_A2B.apply(weights_init_normal)\n",
        "netG_B2A.apply(weights_init_normal)\n",
        "netD_A.apply(weights_init_normal)\n",
        "netD_B.apply(weights_init_normal)\n",
        "\n",
        "# Lossess\n",
        "criterion_GAN = torch.nn.MSELoss()\n",
        "criterion_cycle = torch.nn.L1Loss()\n",
        "criterion_identity = torch.nn.L1Loss()\n",
        "\n",
        "# Optimizers & LR schedulers\n",
        "optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n",
        "                                lr=opt.lr, betas=(0.5, 0.999))\n",
        "optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n",
        "optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n",
        "\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n",
        "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n",
        "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step)\n",
        "\n",
        "# Inputs & targets memory allocation\n",
        "Tensor = torch.cuda.FloatTensor if opt.cuda else torch.Tensor\n",
        "input_A = Tensor(opt.batchSize, opt.input_nc, opt.size, opt.size)\n",
        "input_B = Tensor(opt.batchSize, opt.output_nc, opt.size, opt.size)\n",
        "target_real = Variable(Tensor(opt.batchSize).fill_(1.0), requires_grad=False)\n",
        "target_fake = Variable(Tensor(opt.batchSize).fill_(0.0), requires_grad=False)\n",
        "\n",
        "fake_A_buffer = ReplayBuffer()\n",
        "fake_B_buffer = ReplayBuffer()\n",
        "\n",
        "# Dataset loader\n",
        "transforms_ = []\n",
        "#[ transforms.Resize(int(opt.size*1.12), Image.BICUBIC), \n",
        "#                transforms.RandomCrop(opt.size), \n",
        "#                transforms.RandomHorizontalFlip(),\n",
        "#                transforms.ToTensor(),\n",
        "#                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
        "dataloader = DataLoader(ImageDataset(opt.dataroot, transforms_=transforms_, unaligned=True), \n",
        "                        batch_size=opt.batchSize, shuffle=True, num_workers=opt.n_cpu)\n",
        "\n",
        "# Loss plot\n",
        "logger = Logger(opt.n_epochs, len(dataloader))\n",
        "###################################\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cycleNew/utilities.py:113: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
            "  torch.nn.init.normal(m.weight.data, 0.0, 0.02)\n",
            "Setting up a new session...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connection.py\", line 159, in _new_conn\n",
            "    (self._dns_host, self.port), self.timeout, **extra_kw)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/util/connection.py\", line 80, in create_connection\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/util/connection.py\", line 70, in create_connection\n",
            "    sock.connect(sa)\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
            "    chunked=chunked)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 354, in _make_request\n",
            "    conn.request(method, url, **httplib_request_kw)\n",
            "  File \"/usr/lib/python3.7/http/client.py\", line 1277, in request\n",
            "    self._send_request(method, url, body, headers, encode_chunked)\n",
            "  File \"/usr/lib/python3.7/http/client.py\", line 1323, in _send_request\n",
            "    self.endheaders(body, encode_chunked=encode_chunked)\n",
            "  File \"/usr/lib/python3.7/http/client.py\", line 1272, in endheaders\n",
            "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
            "  File \"/usr/lib/python3.7/http/client.py\", line 1032, in _send_output\n",
            "    self.send(msg)\n",
            "  File \"/usr/lib/python3.7/http/client.py\", line 972, in send\n",
            "    self.connect()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connection.py\", line 181, in connect\n",
            "    conn = self._new_conn()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connection.py\", line 168, in _new_conn\n",
            "    self, \"Failed to establish a new connection: %s\" % e)\n",
            "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f17b0437450>: Failed to establish a new connection: [Errno 111] Connection refused\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/adapters.py\", line 449, in send\n",
            "    timeout=timeout\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 638, in urlopen\n",
            "    _stacktrace=sys.exc_info()[2])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/util/retry.py\", line 399, in increment\n",
            "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
            "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f17b0437450>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/visdom/__init__.py\", line 711, in _send\n",
            "    data=json.dumps(msg),\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/visdom/__init__.py\", line 677, in _handle_post\n",
            "    r = self.session.post(url, data=data)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 578, in post\n",
            "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 530, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 643, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/adapters.py\", line 516, in send\n",
            "    raise ConnectionError(e, request=request)\n",
            "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f17b0437450>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "[Errno 99] Cannot assign requested address\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception in user code:\n",
            "------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Errno 99] Cannot assign requested address\n",
            "[Errno 99] Cannot assign requested address\n",
            "[Errno 99] Cannot assign requested address\n",
            "[Errno 99] Cannot assign requested address\n",
            "Visdom python client failed to establish socket to get messages from the server. This feature is optional and can be disabled by initializing Visdom with `use_incoming_socket=False`, which will prevent waiting for this request to timeout.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01-OTazm6SLW"
      },
      "source": [
        "\n",
        "\n",
        "###### Training ######\n",
        "for epoch in range(opt.epoch, opt.n_epochs):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        # Set model input\n",
        "        real_A = Variable(input_A.copy_(batch['A']))\n",
        "        real_B = Variable(input_B.copy_(batch['B']))\n",
        "\n",
        "        ###### Generators A2B and B2A ######\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Identity loss\n",
        "        # G_A2B(B) should equal B if real B is fed\n",
        "        same_B = netG_A2B(real_B)\n",
        "        loss_identity_B = criterion_identity(same_B, real_B)*5.0\n",
        "        # G_B2A(A) should equal A if real A is fed\n",
        "        same_A = netG_B2A(real_A)\n",
        "        loss_identity_A = criterion_identity(same_A, real_A)*5.0\n",
        "\n",
        "        # GAN loss\n",
        "        fake_B = netG_A2B(real_A)\n",
        "        pred_fake = netD_B(fake_B)\n",
        "        loss_GAN_A2B = criterion_GAN(pred_fake, target_real)\n",
        "\n",
        "        fake_A = netG_B2A(real_B)\n",
        "        pred_fake = netD_A(fake_A)\n",
        "        loss_GAN_B2A = criterion_GAN(pred_fake, target_real)\n",
        "\n",
        "        # Cycle loss\n",
        "        recovered_A = netG_B2A(fake_B)\n",
        "        loss_cycle_ABA = criterion_cycle(recovered_A, real_A)*10.0\n",
        "\n",
        "        recovered_B = netG_A2B(fake_A)\n",
        "        loss_cycle_BAB = criterion_cycle(recovered_B, real_B)*10.0\n",
        "\n",
        "        # Total loss\n",
        "        loss_G = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
        "        loss_G.backward()\n",
        "        \n",
        "        optimizer_G.step()\n",
        "        ###################################\n",
        "\n",
        "        ###### Discriminator A ######\n",
        "        optimizer_D_A.zero_grad()\n",
        "\n",
        "        # Real loss\n",
        "        pred_real = netD_A(real_A)\n",
        "        loss_D_real = criterion_GAN(pred_real, target_real)\n",
        "\n",
        "        # Fake loss\n",
        "        fake_A = fake_A_buffer.push_and_pop(fake_A)\n",
        "        pred_fake = netD_A(fake_A.detach())\n",
        "        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
        "\n",
        "        # Total loss\n",
        "        loss_D_A = (loss_D_real + loss_D_fake)*0.5\n",
        "        loss_D_A.backward()\n",
        "\n",
        "        optimizer_D_A.step()\n",
        "        ###################################\n",
        "\n",
        "        ###### Discriminator B ######\n",
        "        optimizer_D_B.zero_grad()\n",
        "\n",
        "        # Real loss\n",
        "        pred_real = netD_B(real_B)\n",
        "        loss_D_real = criterion_GAN(pred_real, target_real)\n",
        "        \n",
        "        # Fake loss\n",
        "        fake_B = fake_B_buffer.push_and_pop(fake_B)\n",
        "        pred_fake = netD_B(fake_B.detach())\n",
        "        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
        "\n",
        "        # Total loss\n",
        "        loss_D_B = (loss_D_real + loss_D_fake)*0.5\n",
        "        loss_D_B.backward()\n",
        "\n",
        "        optimizer_D_B.step()\n",
        "        ###################################\n",
        "\n",
        "        # Progress report (http://localhost:8097)\n",
        "        logger.log({'loss_G': loss_G, 'loss_G_identity': (loss_identity_A + loss_identity_B), 'loss_G_GAN': (loss_GAN_A2B + loss_GAN_B2A),\n",
        "                    'loss_G_cycle': (loss_cycle_ABA + loss_cycle_BAB), 'loss_D': (loss_D_A + loss_D_B)}, \n",
        "                    images={'real_A': real_A, 'real_B': real_B, 'fake_A': fake_A, 'fake_B': fake_B})\n",
        "\n",
        "    # Update learning rates\n",
        "    lr_scheduler_G.step()\n",
        "    lr_scheduler_D_A.step()\n",
        "    lr_scheduler_D_B.step()\n",
        "\n",
        "    # Save models checkpoints\n",
        "    torch.save(netG_A2B.state_dict(), 'output/netG_A2B.pth')\n",
        "    torch.save(netG_B2A.state_dict(), 'output/netG_B2A.pth')\n",
        "    torch.save(netD_A.state_dict(), 'output/netD_A.pth')\n",
        "    torch.save(netD_B.state_dict(), 'output/netD_B.pth')\n",
        "###################################"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}